<h1>Analysing the Bundestagreden using NLP techniques</h1>

<h2>Abstract</h2>

<h2>Introduction</h2>

    <p>Natural language processing is an important tool in helping machines understand and interpret complex sequences of text. Using these interpretations, 
        predictions can be made on what the text is trying to convey. This has been demonstrated in the use of text summarization and keyword extraction. 
        Using these techniques, large texts can be analysed and used to find more abstract meanings. This paper discusses the use of natural language processing on the Bundesreden and 
        Coalition documents to predict potential projects that PD could be tasked with. </p>

<h2>Method</h2>

    <h3>Inspecting the data</h3>

        <p>The data used consists of a project databank of the PD past projects. Within the dataset there is a project title, description, and department of each project along with inception 
            and completion dates if applicable. This data can be used to create examples of some of the projects that could be extracted from the Bundesreden. It can also be used to embed the 
            descriptions and create embeddings which can then be compared with speeches to identify where the projects could have come from. Finally, the projects can be visualised by projecting 
            the high dimensional embeddings into a lower three-dimensional vector.</p>

        <p>The second dataset used consists of the Bundesreden. It has entries for the time the speech occurred and the date along with a web link to a pdf which contains the transcript of the speech. 
            The data begins in 1949 meaning it is much larger than needed so changing the time frame to a more appropriate time was needed to get meaningful results. When looking at the pdf documents 
            it is seen that they are extremely long and therefore some summarisation or keyword/keyphrase extraction is needed to condense the document down to get a better idea of what ideas are represented 
            within it.</p>

        <p>Finally, a dataset was created by the author to list the departments at PD and have a description for each taken from the departments themselves. This dataset was used to be able to categorise which 
            departments could be expecting potential projects predicted. It can also be used to see whether existing projects are similar to their respective department and see how relevant they are to each other.
        </p>

    <h3>Text Extraction</h3>

        <p>Text extraction is essential as if the pdf documents were used, nothing could be used to put into an nlp algorithm. Therefore, extracting the text into a text file and then reading it is necessary to 
            be able to easily access the data and pass it through various neural networks or algorithms to extract information. A function was written to download pdf files and save them from a URL. A second function 
            was written to read the pdf and save the contents as a pdf file where each new line represents a sentence. These functions allow for all relevant speeches to be downloaded and made into a format that can 
            be entered into a neural network for training or embedding.</p>

    <h3>Text preparation and pre processing</h3>

        <p>After the text has been extracted from the pdf document it still has many issues with it which need to be resolved before it can be given to a network. This includes punctuation, special characters and such 
            which don’t give the text any further information. These need to be removed so that the network can use the useful information without being confused or misled by these characters.</p>

        <p>Stop words are words such as … which are common and don’t help with understanding the context or sentiment behind a piece of text. This means they need to be removed which will help with the text summarisation 
            and keyword extraction as it removes common words and some keyword extraction algorithms use common words to decide what they want to use. The text also needs to be lemmatized meaning that all words are put 
            into their base form. This allows similar words to be grouped together so that a network can see which words are most common and therefore most likely to be potential key words or phrases. In German accents 
            are used so these need to be filtered out as they are considered special characters and can be easily substituted.</p>

        <p>Finally, the text needs to be tokenised. This means splitting up the words so they can be seen as individual words within sentences instead of a continuous string of characters. This will allow a neural network 
            to embed each word individually and compare them for applications such as predicting the next word or understanding the text and producing text from this.</p>

    <h3>Transformers</h3>

        <p>A transformer is a deep learning technique which uses the mechanism of attention. This means that it places emphasis on specific words allowing for the model to consider context around key words allowing it to 
            understand the text better and how words fit withing sentences. Transformers take in a sequence of data, such as a text document however doesn’t always process the data in that order. As it uses attention, 
            certain words have more emphasis and are ‘remembered’ while less important words such as ‘der’ are not given much attention as they do not contribute to the understanding of the text. Transformers are used 
            as a replacement for recurrent neural networks which will not be explained in this paper however for tasks that this paper concerns, using transformers will produce better results due to the superior text 
            reasoning and understanding transformers possess.</p>

    <h3>BERT</h3>

        <p>In 2018, Devlin et al. created and published a transformer machine learning technique called Bidirectional Encoder Representations from Transformers or BERT which was developed for Google. It has been shown 
            to be very good at keyword extraction and other natural language understanding tasks. This model was therefore used for some of the applications which involved word embedding which is a technique of representing 
            words in a high dimensional vector. The bidirectional aspect of the transformer means that it takes into account the whole sentence before determining which part requires the most attention and how words later in 
            the sentence affect words coming before.</p>

    <h3>Keyword Extraction</h3>

        <p>There are two main types of machine learning, supervised and unsupervised learning. The former uses data which has labels meaning that it can be classified. This allows for a neural network or algorithm to be trained
            and it will then produce similar results to those given in training. The latter has no data labels meaning that it is just data. When looking at the data which was given, the speeches are not labelled data as potential 
            projects cannot be assigned to certain speeches. Because the amount of data used is so large, it is not possible to create a labelled dataset without spending significant resources. Therefore, to begin with unsupervised
            machine learning techniques were used.</p>

        <p>The first of these is keyword extraction. Keyword extraction is where the text is given into a neural network or algorithm and keywords or key phrases are extracted. Some techniques use methods such as finding the most 
            common words while others also take the context into account to produce the most important parts of the document.</p>

    <h3>Text Summarisation</h3>

        <p>Another method for extracting key information is text summarisation. The BERT algorithm has been found to be very good at understanding text and being able to analyse texts however struggles with creating new strings of 
            text from it. Text summarisation requires looking at a piece of text and extracting the key information in a useful way. Autoregressive decoders such as the Seq2Seq model however are very good at decoding from vectors 
            to words. Therefore, combining these two models will give us an encoder and decoder which can be used on our text. </p>

        <p>When using the text summarisation functions given by hugging face transformers library [ ], the speeches were passed through which had already been pre-processed by converting from pdf to text. It was found that the 
            function could only take in tokens of 1024, therefore a function was written to split the text into its sentences and then into paragraphs which were no larger than 1024 words. After summarising the text, the document 
            was embedded and compared to embeddings of the different previous projects that PD has completed using the cosine similarity.</p>

    <h3>Visualisations</h3>

        <p>Using the PD website, the different departments in the company were identified and then description of each of these were found. The BERT network was used to embed these words into high dimensional vectors. These higher 
            dimensional vectors are hard to understand therefore dimensional reduction would allow us to transform these higher degree vectors into lower dimensions such as two or three dimensional. This can then be plotted on a 
            scatter plot to see where different departments lie within the lower dimensional vector space.</p>

        <p>The dimensional reduction was achieved using Principal Component Analysis (PCA). This allowed for the higher 728 dimensional vectors to be projected onto 3 dimensions while losing as little accuracy and information as possible.</p>

        <p>A second technique used was using T-SNE. This is a non-linear method of reducing the dimension of data and is better at creating clusters of similar data to get better separation. This is a large advantage over the PCA algorithm 
            however it needs to be calibrated to the data and this can sometimes be challenging to achieve. When trying to calibrate the data for this project the visualisations were difficult to find and thus the PCA algorithm performed 
            better. </p>


<h2>Results</h2>

<h2>Conclusion</h2>