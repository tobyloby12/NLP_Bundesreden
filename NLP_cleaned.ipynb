{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "NLP cleaned.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RCMgfMVrYFi2",
        "Y-N6KJG2a274",
        "UrKcvjz4e0iV",
        "EIAC3kr6Jruu"
      ],
      "authorship_tag": "ABX9TyNFlcv9fuyHD6AZKTV77y1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('bundesreden_env': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "e73953e0a71596262baf353ddd42d6067dee2e92dc224ca6c7af066a3bb81a39"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobyloby12/NLP_Bundesreden/blob/main/NLP_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Connecting to notebook\r\n",
        "\r\n",
        "> Mounting notebook to google drive \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> Changing directory so that I can access data"
      ],
      "metadata": {
        "id": "RCMgfMVrYFi2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "metadata": {
        "id": "MdQi1yUXX26c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a90304-b830-4a18-e743-85857da776a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "\r\n",
        "path = 'gdrive/MyDrive'\r\n",
        "path = os.path.join(path, 'PD NLP project')\r\n",
        "os.chdir(path)"
      ],
      "outputs": [],
      "metadata": {
        "id": "fRZ7A4ShX5SE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing data\r\n",
        "\r\n",
        "> Create a function to take in a link and save it to drive so that I can access it\r\n",
        "\r\n",
        "> Create a function to read through the pdf and create a text file I can use"
      ],
      "metadata": {
        "id": "3_3q8NRmYP8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df_data = pd.read_excel('Datasets/PD-Projektdatenbank.xlsx')\r\n",
        "df_pd = df_data[['Titel', 'Projektbeschreibung', 'Leistungszeitraum']]\r\n",
        "df_speeches = pd.read_csv('Datasets/speeches.csv')\r\n",
        "\r\n",
        "df_speeches.date[2][:-6] == '1949'\r\n",
        "\r\n",
        "date_filter = []\r\n",
        "for line in range(len(df_speeches)):\r\n",
        "  date = int(df_speeches.date[line][:-6])\r\n",
        "  if date > 2000:\r\n",
        "    date_filter.append(True)\r\n",
        "  else:\r\n",
        "    date_filter.append(False)\r\n",
        "\r\n",
        "pd.DataFrame(date_filter)\r\n",
        "speeches_date_filtered = df_speeches[date_filter]"
      ],
      "outputs": [],
      "metadata": {
        "id": "B81xDnn-YNuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text cleaning function"
      ],
      "metadata": {
        "id": "diKdIu5kZsem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import nltk\r\n",
        "from string import punctuation\r\n",
        "from string import digits\r\n",
        "import spacy\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "german_stopwords = stopwords.words('german')\r\n",
        "speech_content = df_speeches[['speechContent']]\r\n",
        "\r\n",
        "\r\n",
        "#setting up stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "german_stopwords = stopwords.words('german')\r\n",
        "model_de = spacy.load('de_core_news_sm')\r\n",
        "\r\n",
        "# removing umlauts function\r\n",
        "def remove_umlauts(word):\r\n",
        "  tempWord = word\r\n",
        "  tempWord = tempWord.replace('ä', 'ae')\r\n",
        "  tempWord = tempWord.replace('ö', 'oe')\r\n",
        "  tempWord = tempWord.replace('ü', 'ue')\r\n",
        "  tempWord = tempWord.replace('Ä', 'Ae')\r\n",
        "  tempWord = tempWord.replace('Ö', 'Oe')\r\n",
        "  tempWord = tempWord.replace('Ü', 'Ue')\r\n",
        "  tempWord = tempWord.replace('ß', 'ss')\r\n",
        "  return tempWord\r\n",
        "\r\n",
        "#remove currency function\r\n",
        "def remove_currency(word):\r\n",
        "  tempWord = word\r\n",
        "  tempWord = tempWord.replace('$', '')\r\n",
        "  tempWord = tempWord.replace('€', '')\r\n",
        "  tempWord = tempWord.replace('¥', '')\r\n",
        "  tempWord = tempWord.replace('₹', '')\r\n",
        "  tempWord = tempWord.replace('£', '')\r\n",
        "  return tempWord\r\n",
        "\r\n",
        "#lemmatization function\r\n",
        "def lemmatizer(text): \r\n",
        "    sent = []\r\n",
        "    doc = model_de(text)\r\n",
        "    for word in doc:\r\n",
        "        sent.append(word.lemma_)\r\n",
        "    return \" \".join(sent)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#full function to clean text from input text\r\n",
        "def data_cleaning(input_text):\r\n",
        "  \r\n",
        "  #removing punctuation\r\n",
        "  remove_pun = str.maketrans('', '', punctuation)\r\n",
        "  text_wo_pun = input_text.translate(remove_pun)\r\n",
        "\r\n",
        "  #removing digits\r\n",
        "  remove_digits = str.maketrans('', '', digits)\r\n",
        "  text_wo_num = text_wo_pun.translate(remove_digits)\r\n",
        "\r\n",
        "  #removing currency\r\n",
        "  text_wo_currency = remove_currency(text_wo_num)\r\n",
        "\r\n",
        "  #lemmatization\r\n",
        "  text_lemmatized = lemmatizer(text_wo_currency.lower())\r\n",
        "\r\n",
        "  #removing stop words\r\n",
        "  text_wo_stop_words = [word for word in text_lemmatized.split() if text_lemmatized.lower() not in german_stopwords_wo_umlaut]\r\n",
        "\r\n",
        "  return text_wo_stop_words\r\n",
        "\r\n",
        "#setting up stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "german_stopwords = stopwords.words('german')\r\n",
        "german_stopwords_wo_umlaut = []\r\n",
        "for word in german_stopwords:\r\n",
        "  german_stopwords_wo_umlaut.append(remove_umlauts(word))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Gebruiker\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Gebruiker\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Gebruiker\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "metadata": {
        "id": "9hN6lmm_aDBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61a3a1d-17a8-4a6f-f2fa-eea94e14c0b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting text from pdf\n",
        "\n",
        "> Creating function to read and write pdf file to drive\n",
        "\n",
        "> Creating function to read pdf and convert it to text"
      ],
      "metadata": {
        "id": "Y-N6KJG2a274"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install pdfplumber;"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.5.28.tar.gz (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20200517\n",
            "  Downloading pdfminer.six-20200517-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 24.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfplumber) (7.1.2)\n",
            "Collecting Wand\n",
            "  Downloading Wand-0.6.7-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (2.4.0)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 35.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20200517->pdfplumber) (3.0.4)\n",
            "Building wheels for collected packages: pdfplumber\n",
            "  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfplumber: filename=pdfplumber-0.5.28-py3-none-any.whl size=32240 sha256=c9b49c90829d3f10f3dc9c7912db4533e6ad8d0c435582b8c46f0bfa91b16835\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/b1/a0/c0a77b756d580f53b3806ae0e0b3ec945a8d05fca1d6e10cc1\n",
            "Successfully built pdfplumber\n",
            "Installing collected packages: pycryptodome, Wand, pdfminer.six, pdfplumber\n",
            "Successfully installed Wand-0.6.7 pdfminer.six-20200517 pdfplumber-0.5.28 pycryptodome-3.10.1\n"
          ]
        }
      ],
      "metadata": {
        "id": "Yr3GunRDcNOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c661acdb-b5d5-4675-c17d-d82e1d672671"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import os\r\n",
        "import urllib.request\r\n",
        "import pdfplumber\r\n",
        "\r\n",
        "def download_pdf(pdf_url, filename):\r\n",
        "  response = urllib.request.urlopen(pdf_url)\r\n",
        "  file = open(filename + \".pdf\", 'wb')\r\n",
        "  file.write(response.read())\r\n",
        "  file.close()\r\n",
        "\r\n",
        "def convert_pdf_to_text(filename):\r\n",
        "  with pdfplumber.open(filename + '.pdf') as pdf:\r\n",
        "    without_cleaning = ''\r\n",
        "    for i in range(len(pdf.pages)-1):\r\n",
        "      page = pdf.pages[i]\r\n",
        "      if page.extract_text() != None:\r\n",
        "        without_cleaning = without_cleaning + page.extract_text()\r\n",
        "    return without_cleaning"
      ],
      "outputs": [],
      "metadata": {
        "id": "VIajVbiIa2UG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "pdf_path = 'pdf_files'\r\n",
        "path = os.path.join(os.getcwd(), pdf_path)\r\n",
        "\r\n",
        "#pdfurl = 'https://dip21.bundestag.de/dip21/btp/15/15014.pdf'\r\n",
        "filename = 'pdf_files/testfile'\r\n",
        "\r\n",
        "#download_pdf(pdfurl, filename)\r\n",
        "without_cleaning = convert_pdf_to_text(filename)\r\n",
        "\r\n",
        "#with_cleaning = data_cleaning(without_cleaning)"
      ],
      "outputs": [],
      "metadata": {
        "id": "DF81YJ51cN9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trying out BERT with hugging face library\n",
        "https://www.machinecurve.com/index.php/2020/12/21/easy-text-summarization-with-huggingface-transformers-and-machine-learning/"
      ],
      "metadata": {
        "id": "UrKcvjz4e0iV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "!pip install transformers;\r\n",
        "from transformers import pipeline\r\n",
        "\r\n",
        "with_cleaning_small = ''\r\n",
        "for i in range(400):\r\n",
        "  with_cleaning_small += with_cleaning[i] + ' '\r\n",
        "\r\n",
        "# Open and read the article\r\n",
        "#f = open(\"article.txt\", \"r\", encoding=\"utf8\")\r\n",
        "to_tokenize = with_cleaning\r\n",
        "\r\n",
        "# Initialize the HuggingFace summarization pipeline\r\n",
        "summarizer = pipeline(\"summarization\")\r\n",
        "summarized = summarizer(to_tokenize, min_length=75, max_length=300)\r\n",
        "\r\n",
        "# Print summarized text\r\n",
        "print(summarized)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from transformers) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-win_amd64.whl (2.0 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from transformers) (2021.8.21)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp39-cp39-win_amd64.whl (213 kB)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from transformers) (4.62.2)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: requests in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from transformers) (2.26.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from transformers) (1.20.3)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\gebruiker\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from requests->transformers) (3.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from requests->transformers) (1.26.6)\n",
            "Requirement already satisfied: click in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\gebruiker\\.conda\\envs\\bundesreden_env\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in c:\\users\\gebruiker\\appdata\\roaming\\python\\python39\\site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: filelock, tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "Successfully installed filelock-3.0.12 huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'with_cleaning' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-c6179b1dd517>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwith_cleaning_small\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m   \u001b[0mwith_cleaning_small\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mwith_cleaning\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Open and read the article\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'with_cleaning' is not defined"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "ny6Jr6Bzc-9B",
        "outputId": "164dbb00-f5ab-4760-de4a-126246bf1903"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "without_cleaning_split = []\r\n",
        "tokens = without_cleaning.split()\r\n",
        "token_length = 350\r\n",
        "for i in range(len(tokens)//token_length):\r\n",
        "  sentance = ''\r\n",
        "  for j in range(token_length):\r\n",
        "    sentance += tokens[token_length*i + j] + ' '\r\n",
        "  without_cleaning_split.append(sentance)"
      ],
      "outputs": [],
      "metadata": {
        "id": "rYLPJ0jLe7k_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "summary = []\r\n",
        "for sentance in without_cleaning_split:\r\n",
        "  to_tokenize = sentance\r\n",
        "  print('done')\r\n",
        "  # Initialize the HuggingFace summarization pipeline\r\n",
        "  summarizer = pipeline(\"summarization\")\r\n",
        "  summarized = summarizer(to_tokenize, min_length=10, max_length=300)\r\n",
        "  summary.append(summarized)\r\n",
        "  # Print summarized text\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "ngrF0Mfre86p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(summary)\r\n",
        "print(len(summary))"
      ],
      "outputs": [],
      "metadata": {
        "id": "j-fpX424e-cE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install transformers"
      ],
      "outputs": [],
      "metadata": {
        "id": "PaRIsBqOhp87"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from transformers import pipeline\r\n",
        "\r\n",
        "to_tokenize = without_cleaning\r\n",
        "\r\n",
        "# Initialize the HuggingFace summarization pipeline\r\n",
        "summarizer = pipeline(\"summarization\")\r\n",
        "summarized = summarizer(to_tokenize, min_length=75, max_length=300)\r\n",
        "\r\n",
        "# Print summarized text\r\n",
        "print(summarized)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mvxpTKh-hs8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#My own version using hugging transformers"
      ],
      "metadata": {
        "id": "EIAC3kr6Jruu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 26.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 28.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 25.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fOLKeNpJzLa",
        "outputId": "2d0dd9c1-6d73-4ee3-fa45-26e7f8a1752d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# function to split sentances into paragraphs\r\n",
        "import re\r\n",
        "from string import ascii_lowercase as alphabet\r\n",
        "\r\n",
        "def split_to_sentance(max_sentance_length, text):\r\n",
        "  split_text = re.split(f'[{punctuation}\\n]', text)\r\n",
        "  split_text = [sentance.strip() for sentance in split_text]\r\n",
        "  [split_text.remove(sentance) for sentance in split_text if sentance in ['','f{punctuation}']]\r\n",
        "  \r\n",
        "  \r\n",
        "  \r\n",
        "  paragraphs = ['' for i in range(len(text.split())//max_sentance_length + 1)]\r\n",
        "  i = 0\r\n",
        "  for sentance in split_text:\r\n",
        "    sentance_length = len(sentance.split())\r\n",
        "\r\n",
        "    if len(paragraphs[i].split()) < (max_sentance_length - sentance_length):\r\n",
        "      paragraphs[i] += ' ' + sentance\r\n",
        "    else:\r\n",
        "      i += 1\r\n",
        "  return paragraphs\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "6oFY54d-jl19"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text = split_to_sentance(350, without_cleaning)\r\n",
        "len(text[0].split())"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "349"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13fwW2Yv_MSW",
        "outputId": "c941d359-7777-4ecb-b0e7-c5a98a79f615"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from transformers import pipeline\r\n",
        "\r\n",
        "#file = open('summary.txt', 'w')\r\n",
        "file.write('')\r\n",
        "i = 0\r\n",
        "total = len(without_cleaning.split())\r\n",
        "for paragraph in text:\r\n",
        "  to_tokenize = paragraph\r\n",
        "  print(i/total * 100)\r\n",
        "\r\n",
        "  # Initialize the HuggingFace summarization pipeline\r\n",
        "  summarizer = pipeline(\"summarization\")\r\n",
        "  summarized = summarizer(to_tokenize, min_length=1, max_length=20)\r\n",
        "  file = open('summary.txt', 'a')\r\n",
        "  file.write(summarized[0]['summary_text'] + '\\n')\r\n",
        "  i = i + len(paragraph.split())\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "MQIFVH4q7ibT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "file = open('summary.txt', 'r')\r\n",
        "\r\n",
        "text = ''\r\n",
        "for line in file:\r\n",
        "  text += line + ' '"
      ],
      "outputs": [],
      "metadata": {
        "id": "WKjC8N_9R66i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "len(text.split())"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "Session cannot generate requests",
          "traceback": [
            "Error: Session cannot generate requests",
            "at w.executeCodeCell (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:325139)",
            "at w.execute (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:324460)",
            "at w.start (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:320276)",
            "at runMicrotasks (<anonymous>)",
            "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
            "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334803)",
            "at async t.CellExecutionQueue.start (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334343)"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T3y2sLrE2kq",
        "outputId": "d3d1fa36-4ea5-4c4b-cf9e-6a0161c0c96d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(text)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "Session cannot generate requests",
          "traceback": [
            "Error: Session cannot generate requests",
            "at w.executeCodeCell (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:325139)",
            "at w.execute (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:324460)",
            "at w.start (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:320276)",
            "at runMicrotasks (<anonymous>)",
            "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
            "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334803)",
            "at async t.CellExecutionQueue.start (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334343)"
          ]
        }
      ],
      "metadata": {
        "id": "SCKxm1GhDRKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def summarisation(filename, input_text, total_length, MIN_length = 1, MAX_length = 300):\r\n",
        "  file = open(filename, 'w')\r\n",
        "  file.write('')\r\n",
        "  i = 0\r\n",
        "  total = total_length\r\n",
        "  for paragraph in input_text:\r\n",
        "    to_tokenize = paragraph\r\n",
        "    print(i/total * 100)\r\n",
        "\r\n",
        "    # Initialize the HuggingFace summarization pipeline\r\n",
        "    summarizer = pipeline(\"summarization\")\r\n",
        "    summarized = summarizer(to_tokenize, min_length=MIN_length, max_length=MAX_length)\r\n",
        "    file = open(filename, 'a')\r\n",
        "    file.write(summarized[0]['summary_text'] + '\\n')\r\n",
        "    i = i + len(paragraph.split())"
      ],
      "outputs": [],
      "metadata": {
        "id": "b7teLqZxEZSO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "new_summarisation = split_to_sentance(350, text)\r\n",
        "summarisation('second_summary.txt', new_summarisation, len(text.split()), 30, 100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "17.365269461077844\n",
            "34.78043912175649\n",
            "52.09580838323353\n",
            "69.311377245509\n",
            "86.62674650698602\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5OqC1VCE75h",
        "outputId": "c1aa0900-8582-4104-8577-f8c41574f240"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "file = open('second_summary.txt', 'r')\r\n",
        "\r\n",
        "text_2 = ''\r\n",
        "for line in file:\r\n",
        "  text_2 += line + ' '"
      ],
      "outputs": [],
      "metadata": {
        "id": "OK6iYaANGK5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(text_2)\r\n",
        "print(len(text_2.split()))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inhalt Wahl des Abgeordneten Dieter Wiefelsp Antrag der Fraktionen der SPD der CDU CSU des B� The 15 Wahlperiode – 14 Sitzung Berlin Donnerstag . Operation Fox soll schließlich die in keiten abgeschloss Mazedonien darf nicht Beifall bei der SPD und dem Die CDUCSU wird der Verlä\n",
            "  Steuerreform wollen 2004 und 2005 Entlastungen für Unter Unser Land braucht dringend  des Bundes eingeschlagen Wir brauchen hierzulande in Europa and dieBayerischeSt Die longue übrigens immer noch der Wachstums In unserer Außenwirtschaftspolitik wollen wir Deutsche Gewerkschaftsbund in der begin\n",
            "  Die CDU CSU macht es entschieden zu Ausfr Die Personal Service Agenturenkönnen auf der Ba Beifall des Im Vermittlungsausschuss wollen Sie mit uns zusam Hartz ist eine revolutionäre Idee – die Mehrheit in BeifAll bei der SPD and dem BÜNDNIS 90 Nord Regierung war wie sie sich prä\n",
            "  Dr Jürgen Gehb CDU CSU fassungswidrigke Untersuchungsausschuss in allen Phasen des Par n Schindluder with der Verfassung getrieben werde Plenarsaal möglichst zügig zu ver Die Rentenversicherung macht im Moment wie  konj In der ersten drei Quar rund 77\n",
            "  Aussagen zur Sozialversicherung geboten hat Deutscher Bundestag – 15 Wahlperiode – 14 S Im Oktober2003 wird die Mindestreserve auf ei d Aufgrund der ne gestiegene Motivation weiter zu steig Die Integrationsfachdienste haben im Zusammenspiel Reform der sozialen Sicherungssysteme\n",
            "  Manfred Grund CDU CSU Auch da hat der Kanzler Recht  schäftsbereic ist nichts anderes als Augenwischerei Eigentum in unserem Land ungerecht und ungleich Her Beifall bei  bei der CDUCSU – Ute Kumpf SPD Herta Eymer Lübeck .\n",
            " \n",
            "247\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogdg2hIKGb1j",
        "outputId": "d6673730-b813-4c83-e6e8-913b131ec286"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "AkMgL6urJF8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cosine similarity matching"
      ],
      "metadata": {
        "id": "pkhuSnozJMzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install spacy\r\n",
        "!python -m spacy download de_core_news_sm\r\n",
        "\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words = stopwords.words(\"german\")\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "6eNtEKRRJY-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install sentence_transformers\r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "\r\n",
        "model = SentenceTransformer('msmarco-distilbert-multilingual-en-de-v2-tmp-trained-scratch')\r\n",
        "doc_embedding = model.encode([text_2])"
      ],
      "outputs": [],
      "metadata": {
        "id": "v0r0XghaUl4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "outputs": [],
      "metadata": {
        "id": "E4Mq6FFlUvun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "potential_projects_series = df_pd[['Titel']]\r\n",
        "potential_projects = []\r\n",
        "potential_projects_embeddings = []\r\n",
        "for i in range(len(potential_projects_series)):\r\n",
        "  potential_projects.append(potential_projects_series.iloc[i])\r\n",
        "\r\n",
        "for i in range(len(potential_projects)):\r\n",
        "  embeddings = model.encode(potential_projects[i])\r\n",
        "  potential_projects_embeddings.append(embeddings)\r\n",
        "\r\n",
        "similarities = []\r\n",
        "for i in range(len(potential_projects_embeddings)):\r\n",
        "  similarities.append(cosine_similarity(doc_embedding, potential_projects_embeddings[i]).astype(float))\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "_ITRX1rwU63S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "potential_projects_series['Similarities'] = similarities"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzVbqySXZdru",
        "outputId": "2248f105-32c7-44ac-af3c-b08bfa2677e0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "potential_projects_series = potential_projects_series.sort_values('Similarities')"
      ],
      "outputs": [],
      "metadata": {
        "id": "wnCh4eckVXRK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "potential_projects_series.to_csv('projects_similarities.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "gJ2g-dYFcE6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "Session cannot generate requests",
          "traceback": [
            "Error: Session cannot generate requests",
            "at w.executeCodeCell (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:325139)",
            "at w.execute (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:324460)",
            "at w.start (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:320276)",
            "at runMicrotasks (<anonymous>)",
            "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
            "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334803)",
            "at async t.CellExecutionQueue.start (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334343)"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trying with coalition agreements"
      ],
      "metadata": {
        "id": "A70aFH8z_WGZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "from transformers import pipeline\r\n",
        "file_url = 'https://www.bundesregierung.de/resource/blob/974430/847984/5b8bc23590d4cb2892b31c987ad672b7/2018-03-14-koalitionsvertrag-data.pdf'\r\n",
        "filename = 'pdf_files/19th-coalition'\r\n",
        "#download_pdf(file_url, filename)\r\n",
        "coalition_text = convert_pdf_to_text(filename)\r\n",
        "coalition_text_split = split_to_sentance(300, coalition_text)\r\n",
        "summarisation('coalition_summary_second_try.txt', coalition_text_split, len(coalition_text.split()), 1, 15)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "HmTrl8G3cUEp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "73a9f3fc-e013-4be3-e757-c235057b6677"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "file = open('coalition_summary_second_try.txt', 'r')\r\n",
        "text = ''\r\n",
        "for line in file:\r\n",
        "  text += line\r\n",
        "\r\n",
        "\r\n",
        "len(text.split())"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "Session cannot generate requests",
          "traceback": [
            "Error: Session cannot generate requests",
            "at w.executeCodeCell (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:325139)",
            "at w.execute (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:324460)",
            "at w.start (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:320276)",
            "at runMicrotasks (<anonymous>)",
            "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
            "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334803)",
            "at async t.CellExecutionQueue.start (c:\\Users\\Gebruiker\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.1195043623\\out\\client\\extension.js:90:334343)"
          ]
        }
      ],
      "metadata": {
        "id": "f6lxmQKkA00W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#need to do cosine similarity on this"
      ],
      "outputs": [],
      "metadata": {
        "id": "_3TJMngIPw_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web scraping to try create dataset with unique PD departments"
      ],
      "metadata": {
        "id": "dF6vEzN3PiMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "departments = ['MB Bundes- und Landesbau', 'MB Gesundheitswesen', 'MB Kommunalberatung', 'CC beschaffung und Vergabe', 'CC Lebensyzklusorientiertes Projektmanagement, Nachhaltigkeit', 'CC Transformation', 'CC Wirtschaftlichkeit und Finanyen',\r\n",
        "               'PG Building Information Modeling', 'PG Kommunales E-Government', 'PG Mediyintechnik', 'PG Smart City, Smart Data', 'PG Wohnen und Quartier']"
      ],
      "outputs": [],
      "metadata": {
        "id": "UZTJl-D6P7Oe"
      }
    }
  ]
}